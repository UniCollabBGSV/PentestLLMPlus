import dataclasses
import os
import re
import time
from typing import Any, Dict, List, Tuple

import loguru
from tenacity import *

from pentestgpt.utils.llm_api import LLMAPI

logger = loguru.logger
logger.remove()

import ollama
from ollama import Client 

@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None


@dataclasses.dataclass
class Conversation:
    conversation_id: str = None
    message_list: List[Message] = dataclasses.field(default_factory=list)

    def __hash__(self):
        return hash(self.conversation_id)

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id
    

class OllamaAPI(LLMAPI):
    def __init__(self, config_class, use_langfuse_logging=False):
        self.name = str(config_class.model)
        self.model = config_class.model
        self.log_dir = config_class.log_dir
        self.history_length = 5  # maintain 5 messages in the history. (5 chat memory)
        self.conversation_dict: Dict[str, Conversation] = {}
        self.token_limit = config_class.context_length_default 

        self.client = Client(host=config_class.api_base)
        self.pull_model(self.model)
        self.get_context_length()
        self.chat_options = config_class.chat_options
        self.chat_options["num_ctx"] = self.token_limit

    def pull_model(self, model):
        try:
            self.client.chat(model)
        except ollama.ResponseError as e:
            print('Error:', e.error)
            if e.status_code == 404:
                self.client.pull(model)

    def get_context_length(self):
        model_info = self.client.show(self.model)['model_info']
        key_list = [k for k in model_info.keys() if ".context_length" in k]
        if len(key_list) == 1:
            self.token_limit = model_info[key_list[0]]
        else:
            logger.warning(f"Cannot find the context length in the model info. Use the default value {self.token_limit}.")


    def _token_compression(self, message) -> str:
        # send a separate API request to compress the message
        chat_message = [
            {
                "role": "system",
                "content": "You are a helpful assistant.",
            },
            {
                "role": "user",
                "content": f"{message} \n Please reduce the word count of the given message to save tokens. Keep its original meaning so that it can be understood by a large language model.",
            },
        ]
        compressed_message = self._chat_completion(chat_message)
        return compressed_message


    def _chat_completion_fallback(self, history: List) -> str:
        response = self.client.chat(
            model=self.model, 
            messages=history)
        return response["message"]["content"]
    

    def _chat_completion(
        self, history: List, model="llama3.2"
    ) -> str:
        if not hasattr(self, "model"):
            self.model = model
        try:
            if self._count_token(history) > self.token_limit:
                logger.warning("Token size limit reached. The recent message is compressed")
                ## 1. compress the last message
                history[-1]["content"] = self._token_compression(history[-1]["content"])
               
                ## 2. reduce the number of messages in the history. Minimum is 2
                if self.history_length > 2:
                    self.history_length -= 1
               
                ## update the history
                history = history[-self.history_length :]
                response = self.client.chat(
                            model=self.model, 
                            messages=history,
                            options= self.chat_options
                        )
            else:
                response = self.client.chat(
                                model=self.model, 
                                messages=history,
                                options= self.chat_options
                            )
        except Exception as e:
            logger.error(e)
            return self._chat_completion_fallback(history)

        return response["message"]["content"]


if __name__ == "__main__":
    from pentestgpt.utils.APIs.module_import import OllamaConfigClass

    local_config_class = OllamaConfigClass()
    local_config_class.log_dir = "logs"
    client = Client(host=local_config_class.api_base)
    # model = OllamaAPI(local_config_class)
    
    # test is below
    # 1. create a new conversation
    content = """You're an excellent cybersecurity penetration tester assistant. 
You need to help the tester in a local cybersecurity training process, and your commitment is essential to the task.
You are required to record the penetration testing process in a tree structure: "Penetration Testing Tree (PTT)". It is structured as follows:
(1) The tasks are in layered structure, i.e., 1, 1.1, 1.1.1, etc. Each task is one operation in penetration testing; task 1.1 should be a sub-task of task 1.
(2) Each task has a completion status: to-do, completed, or not applicable.
(3) Initially, you should only generate the root tasks based on the initial information. In most cases, it should be reconnaissance tasks. You don't generate tasks for unknown ports/services. You can expand the PTT later."""*100
    # content = "Who is Ho Chi Minh?"
    result = client.chat(
        model=local_config_class.model, 
        messages=[{'role': 'user', 'content': content}],
        options= {
            "num_ctx": 16000,
            "num_predict": 32000
        })
    print("Answer 1")
    print(result)
    # print(f"num tokens =  {model._count_token(content)}")
    exit()
    print(result["message"]["content"])
    print('END')
    
